{
  "bsId": "BS-001",
  "prdId": "PRD-001",
  "version": "1.0.0",
  "status": "draft",
  "title": "Build Sequence: PRD-001 (Part 1 of 8)",
  "buildPhilosophy": "Generated using vertical-slice strategy. Each FBS represents a testable, deployable increment.",
  "generationStrategy": "vertical-slice",
  "buildSequence": [
    {
      "id": "FBS-001",
      "title": "Core Architecture & Monorepo Setup",
      "summary": "Establishes the foundational pnpm workspace monorepo structure with proper application and package organization. Creates the complete directory structure for all applications (api-server, job-crawler, web-ui, n8n-workflows) and shared packages under @jobsearch namespace with TypeScript strict mode configuration.",
      "storyScope": [
        {
          "usId": "US-148",
          "acIds": [
            "AC-665",
            "AC-666",
            "AC-667"
          ]
        },
        {
          "usId": "US-149",
          "acIds": [
            "AC-668",
            "AC-669",
            "AC-670",
            "AC-671",
            "AC-672"
          ]
        },
        {
          "usId": "US-150",
          "acIds": [
            "AC-673",
            "AC-674",
            "AC-675",
            "AC-676",
            "AC-677",
            "AC-678"
          ]
        },
        {
          "usId": "US-151",
          "acIds": [
            "AC-679",
            "AC-680",
            "AC-681"
          ]
        }
      ],
      "dependencies": [],
      "testableOutcomes": [
        "Root package.json contains pnpm workspace configuration with apps/* and packages/* patterns",
        "Running pnpm install successfully resolves and links all workspace dependencies",
        "No Turborepo configuration files (turbo.json) exist in the project",
        "All four application directories (api-server, job-crawler, web-ui, n8n-workflows) exist with valid package.json files",
        "All application package names follow @jobsearch/app-name convention",
        "All six shared packages exist under packages/@jobsearch/ with valid package.json files",
        "Root tsconfig.json has strict mode enabled with all strict flags set to true",
        "Running tsc --noEmit on any package produces no type errors",
        "TypeScript compiler options include noImplicitAny, strictNullChecks, and strictFunctionTypes"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 5,
        "contextRequirements": {
          "prdSections": [
            "Architecture Overview",
            "Technical Stack"
          ],
          "tadSections": [
            "System Architecture",
            "Development Environment"
          ],
          "existingModules": [],
          "schemas": []
        },
        "deliverables": [
          "Root package.json with workspace configuration",
          "Complete apps/ directory structure with all application packages",
          "Complete packages/@jobsearch/ directory with all shared packages",
          "Root tsconfig.json with strict TypeScript configuration",
          "Individual package.json files for all apps and packages"
        ]
      },
      "riskLevel": "medium",
      "domain": "Architecture & Code Quality",
      "implementationNotes": "Focus on creating a scalable monorepo structure that supports future microservice extraction. Ensure workspace dependencies use proper protocols and maintain clear separation between applications and shared packages."
    },
    {
      "id": "FBS-002",
      "title": "Code Quality & Package Boundaries",
      "summary": "Implements comprehensive code quality tooling with ESLint and Prettier configuration across all packages. Establishes and enforces clear package boundaries to prevent circular dependencies and enable future microservice extraction.",
      "storyScope": [
        {
          "usId": "US-152",
          "acIds": [
            "AC-682",
            "AC-683",
            "AC-684",
            "AC-685"
          ]
        },
        {
          "usId": "US-153",
          "acIds": [
            "AC-686",
            "AC-687",
            "AC-688",
            "AC-689"
          ]
        }
      ],
      "dependencies": [
        "FBS-001"
      ],
      "testableOutcomes": [
        "Running eslint on any TypeScript file applies linting rules and reports violations",
        "Running prettier --check on any file verifies formatting compliance",
        "Pre-commit hooks or CI checks enforce ESLint and Prettier validation",
        "ESLint configuration includes TypeScript-specific rules for type safety",
        "No circular dependencies exist between any packages in the workspace",
        "Each shared package exports a clear, well-defined public API",
        "Application packages only depend on shared packages and external libraries, not other applications",
        "All workspace dependencies in package.json files use workspace protocol for internal packages"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "small",
        "estimatedHours": 3,
        "contextRequirements": {
          "prdSections": [
            "Code Quality Standards"
          ],
          "tadSections": [
            "Development Standards",
            "Package Architecture"
          ],
          "existingModules": [],
          "schemas": []
        },
        "deliverables": [
          "Root .eslintrc.js configuration",
          "Root .prettierrc configuration",
          "Package boundary validation scripts",
          "Pre-commit hook configuration",
          "Updated package.json files with proper workspace dependencies"
        ]
      },
      "riskLevel": "low",
      "domain": "Architecture & Code Quality",
      "implementationNotes": "Configure ESLint with TypeScript parser and strict rules. Implement dependency analysis tooling to detect and prevent circular dependencies. Ensure all internal package references use workspace protocol."
    },
    {
      "id": "FBS-003",
      "title": "Security & Environment Configuration",
      "summary": "Implements secure environment variable management for API keys and sensitive configuration. Creates comprehensive environment templates and ensures no credentials are exposed in logs or source code.",
      "storyScope": [
        {
          "usId": "US-141",
          "acIds": [
            "AC-638",
            "AC-639",
            "AC-640",
            "AC-641"
          ]
        },
        {
          "usId": "US-142",
          "acIds": [
            "AC-642",
            "AC-643",
            "AC-644"
          ]
        },
        {
          "usId": "US-143",
          "acIds": [
            "AC-645",
            "AC-646",
            "AC-647",
            "AC-648"
          ]
        }
      ],
      "dependencies": [
        "FBS-001"
      ],
      "testableOutcomes": [
        "Application reads API keys exclusively from environment variables on startup",
        "Codebase scan reveals no hardcoded API keys in any source code files",
        "Application fails gracefully with clear error messages when required environment variables are missing",
        "API operations use environment-based keys for authentication without logging actual values",
        "All log output masks or excludes API keys from entries",
        "API error messages and stack traces do not contain API keys",
        "Debug mode logging protects API keys even with verbose output enabled",
        ".env.example file exists in root directory with all required API key variables",
        ".env.example contains clear documentation for obtaining and configuring each API key",
        "Actual .env file is excluded from version control via .gitignore"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 4,
        "contextRequirements": {
          "prdSections": [
            "Security Requirements",
            "External Integrations"
          ],
          "tadSections": [
            "Security Architecture",
            "Configuration Management"
          ],
          "existingModules": [
            "logger",
            "config"
          ],
          "schemas": []
        },
        "deliverables": [
          ".env.example template with all API key variables",
          "Environment configuration validation module",
          "Secure logging configuration that masks sensitive data",
          "Updated .gitignore to exclude .env files",
          "Configuration loading utilities with error handling"
        ]
      },
      "riskLevel": "high",
      "domain": "Deployment & Security",
      "implementationNotes": "Implement robust environment variable validation with clear error messages. Create logging middleware that automatically detects and masks potential API keys. Ensure .env.example is comprehensive and well-documented for developer onboarding."
    },
    {
      "id": "FBS-004",
      "title": "Single-User Authentication & Profile Setup",
      "summary": "Implements localhost-based trust authentication for single-user deployments with one-time profile setup. Establishes environment-based service authentication and automatic single-user mode configuration.",
      "storyScope": [
        {
          "usId": "US-144",
          "acIds": [
            "AC-649",
            "AC-650",
            "AC-651",
            "AC-652"
          ]
        },
        {
          "usId": "US-145",
          "acIds": [
            "AC-653",
            "AC-654",
            "AC-655",
            "AC-656"
          ]
        },
        {
          "usId": "US-146",
          "acIds": [
            "AC-657",
            "AC-658",
            "AC-659",
            "AC-660"
          ]
        },
        {
          "usId": "US-147",
          "acIds": [
            "AC-661",
            "AC-662",
            "AC-663",
            "AC-664"
          ]
        }
      ],
      "dependencies": [
        "FBS-003"
      ],
      "testableOutcomes": [
        "User accessing fresh deployment from localhost is presented with profile setup form",
        "User can complete profile setup and access personalized dashboard on subsequent visits",
        "System automatically trusts localhost connections (127.0.0.1 and ::1) without authentication",
        "System rejects non-localhost requests with 401 Unauthorized response",
        "Services authenticate with each other using environment-based shared secrets",
        "System automatically detects and configures single-user mode on startup",
        "Multi-user features are disabled when single-user mode is active",
        "Profile data is stored using simplified single-user storage model"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 5,
        "contextRequirements": {
          "prdSections": [
            "Authentication",
            "User Management"
          ],
          "tadSections": [
            "User Profile Service",
            "API Gateway"
          ],
          "existingModules": [
            "Environment configuration",
            "Logging utilities"
          ],
          "schemas": [
            "User profile schema",
            "Authentication models"
          ]
        },
        "deliverables": [
          "Authentication middleware for localhost trust",
          "Profile setup API endpoints",
          "Single-user configuration module",
          "Service-to-service authentication utilities",
          "Profile persistence layer"
        ]
      },
      "riskLevel": "medium",
      "domain": "Deployment & Security",
      "implementationNotes": "Focus on localhost detection logic and secure service communication. Ensure graceful fallback when environment variables are missing."
    },
    {
      "id": "FBS-005",
      "title": "Docker Containerization & Infrastructure",
      "summary": "Establishes complete Docker Compose deployment with all application and infrastructure services. Configures proper networking, persistence, and pnpm workspace integration for containerized monorepo structure.",
      "storyScope": [
        {
          "usId": "US-132",
          "acIds": [
            "AC-608",
            "AC-609",
            "AC-610"
          ]
        },
        {
          "usId": "US-133",
          "acIds": [
            "AC-611",
            "AC-612",
            "AC-613"
          ]
        },
        {
          "usId": "US-134",
          "acIds": [
            "AC-614",
            "AC-615",
            "AC-616",
            "AC-617"
          ]
        },
        {
          "usId": "US-135",
          "acIds": [
            "AC-618",
            "AC-619",
            "AC-620",
            "AC-621"
          ]
        }
      ],
      "dependencies": [
        "FBS-001",
        "FBS-002"
      ],
      "testableOutcomes": [
        "Running 'docker-compose up' successfully starts all containers without errors",
        "All services (api-server, job-crawler, web-ui, PostgreSQL, Redis, Ollama, LiteLLM, n8n) are running and healthy",
        "Container logs show no critical startup errors for any service",
        "All monorepo apps build successfully in Docker containers with pnpm workspaces",
        "Shared packages from packages/@jobsearch/ are accessible to all consuming services",
        "API server container exposes endpoints accessible to other services",
        "Job crawler can communicate with api-server and database containers",
        "Web UI connects to api-server and serves user interface",
        "PostgreSQL container initializes with proper persistence",
        "Redis container is accessible for caching operations",
        "Ollama container provides local AI model serving",
        "LiteLLM container provides AI model proxy functionality"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "large",
        "estimatedHours": 8,
        "contextRequirements": {
          "prdSections": [
            "Architecture",
            "Deployment"
          ],
          "tadSections": [
            "All services",
            "Infrastructure components"
          ],
          "existingModules": [
            "Monorepo structure",
            "Package configurations"
          ],
          "schemas": [
            "Database schemas",
            "API contracts"
          ]
        },
        "deliverables": [
          "docker-compose.yml with all services",
          "Dockerfiles for each application service",
          "Docker networking configuration",
          "Volume persistence configuration",
          "Health check configurations",
          "Environment variable templates"
        ]
      },
      "riskLevel": "high",
      "domain": "Deployment & Security",
      "implementationNotes": "Critical to ensure proper pnpm workspace linking in containers and secure inter-service networking. Test thoroughly with clean Docker environment."
    },
    {
      "id": "FBS-006",
      "title": "Core Job Scraping Engine",
      "summary": "Implements the foundational job scraping system with daily automation, multi-platform support, and comprehensive monitoring. Establishes scraping infrastructure for RemoteOK, WeWorkRemotely, Himalayas, and Indeed platforms.",
      "storyScope": [
        {
          "usId": "US-001",
          "acIds": [
            "AC-001",
            "AC-002",
            "AC-003",
            "AC-004",
            "AC-005"
          ]
        },
        {
          "usId": "US-008",
          "acIds": [
            "AC-032",
            "AC-033",
            "AC-034",
            "AC-035",
            "AC-036"
          ]
        }
      ],
      "dependencies": [
        "FBS-003",
        "FBS-005"
      ],
      "testableOutcomes": [
        "System automatically initiates scraping jobs for all platforms at scheduled daily time",
        "Job listings are successfully collected from each platform and stored in database",
        "New scraping cycle triggers automatically after 24 hours from last successful run",
        "System logs execution time, status, and job count for each completed scraping job",
        "Each platform scrapes independently without blocking other platforms",
        "System logs start time, platform, and job identifier when scraping begins",
        "System logs completion metrics including success/failure status and error details",
        "Scraping errors are logged with severity level, message, platform, and timestamp",
        "Failed scraping jobs continue with next platform without blocking entire cycle",
        "Performance metrics (response times, success rates, jobs per platform) are tracked and available"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "large",
        "estimatedHours": 7,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery",
            "Data Processing"
          ],
          "tadSections": [
            "Job Discovery Service",
            "Job Platform Scrapers"
          ],
          "existingModules": [
            "Database models",
            "Logging system",
            "Configuration management"
          ],
          "schemas": [
            "Job listing schema",
            "Scraping metrics schema"
          ]
        },
        "deliverables": [
          "Job scraping orchestration service",
          "Platform-specific scraper modules",
          "Scheduling and automation system",
          "Error handling and retry logic",
          "Monitoring and metrics collection",
          "Job data normalization utilities"
        ]
      },
      "riskLevel": "high",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "Implement respectful rate limiting and robust error handling. Focus on data quality and platform-specific parsing logic. Ensure scraping resilience against website changes."
    },
    {
      "id": "FBS-007",
      "title": "RemoteOK Job Scraper Implementation",
      "summary": "Implements a dedicated scraper module for RemoteOK platform that extracts job listings, normalizes data, and integrates with the core job discovery service. Handles RemoteOK-specific HTML structure parsing and rate limiting.",
      "storyScope": [
        {
          "usId": "US-002",
          "acIds": [
            "AC-006",
            "AC-007",
            "AC-008",
            "AC-009"
          ]
        }
      ],
      "dependencies": [
        "FBS-006"
      ],
      "testableOutcomes": [
        "Scraper successfully retrieves RemoteOK job listings page via HTTP requests",
        "System extracts job title, company, description, location, and posting date from each RemoteOK listing",
        "Duplicate jobs with same title and company are identified and not re-stored in database",
        "All stored job records include 'remoteok' as the source platform identifier",
        "Scraper handles RemoteOK rate limiting and returns appropriate error codes for blocked requests",
        "Invalid or malformed job data is logged and skipped without breaking the scraping process"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 5,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery Service",
            "Platform Integration Requirements"
          ],
          "tadSections": [
            "Job Discovery Service",
            "Job Platform Scrapers"
          ],
          "existingModules": [
            "job-scraping-engine",
            "job-data-normalization"
          ],
          "schemas": [
            "job-listing-schema",
            "platform-metadata-schema"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/platforms/remoteok.ts",
          "packages/@jobsearch/scrapers/src/platforms/remoteok.test.ts",
          "Updated scraper registry configuration",
          "RemoteOK-specific data mapping utilities"
        ]
      },
      "riskLevel": "medium",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "RemoteOK uses dynamic content loading and may require handling of JavaScript-rendered elements. Implement robust CSS selector fallbacks and monitor for layout changes. Consider RemoteOK's API rate limits and implement exponential backoff."
    },
    {
      "id": "FBS-008",
      "title": "WeWorkRemotely Job Scraper Implementation",
      "summary": "Develops a specialized scraper for WeWorkRemotely platform with focus on their category-based job organization and detailed job descriptions. Implements platform-specific parsing logic and error handling.",
      "storyScope": [
        {
          "usId": "US-003",
          "acIds": [
            "AC-010",
            "AC-011",
            "AC-012",
            "AC-013"
          ]
        }
      ],
      "dependencies": [
        "FBS-006"
      ],
      "testableOutcomes": [
        "Scraper successfully retrieves WeWorkRemotely job listings page via HTTP requests",
        "System extracts job title, company, description, location, and posting date from each WeWorkRemotely listing",
        "Duplicate jobs with same title and company are identified and not re-stored in database",
        "All stored job records include 'weworkremotely' as the source platform identifier",
        "Scraper navigates WeWorkRemotely's category-based structure to collect comprehensive job data",
        "System handles WeWorkRemotely's pagination and retrieves jobs from multiple pages"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 4,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery Service",
            "Platform Integration Requirements"
          ],
          "tadSections": [
            "Job Discovery Service",
            "Job Platform Scrapers"
          ],
          "existingModules": [
            "job-scraping-engine",
            "job-data-normalization"
          ],
          "schemas": [
            "job-listing-schema",
            "platform-metadata-schema"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/platforms/weworkremotely.ts",
          "packages/@jobsearch/scrapers/src/platforms/weworkremotely.test.ts",
          "Updated scraper registry configuration",
          "WeWorkRemotely-specific pagination handler"
        ]
      },
      "riskLevel": "medium",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "WeWorkRemotely has a clean HTML structure but uses category-based navigation. Implement efficient category traversal and handle their specific job detail page structure. Monitor for changes in their CSS classes and URL patterns."
    },
    {
      "id": "FBS-009",
      "title": "Himalayas Job Scraper Implementation",
      "summary": "Creates a scraper module for Himalayas platform focusing on startup and remote opportunities. Handles their modern web interface, API endpoints if available, and implements robust data extraction for their job card format.",
      "storyScope": [
        {
          "usId": "US-004",
          "acIds": [
            "AC-014",
            "AC-015",
            "AC-016",
            "AC-017"
          ]
        }
      ],
      "dependencies": [
        "FBS-006"
      ],
      "testableOutcomes": [
        "Scraper successfully retrieves Himalayas job listings page via HTTP requests",
        "System extracts job title, company, description, location, and posting date from each Himalayas listing",
        "Duplicate jobs with same title and company are identified and not re-stored in database",
        "All stored job records include 'himalayas' as the source platform identifier",
        "Scraper handles Himalayas' modern JavaScript-heavy interface and dynamic content loading",
        "System extracts additional metadata specific to Himalayas (startup stage, funding info) when available"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 5,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery Service",
            "Platform Integration Requirements"
          ],
          "tadSections": [
            "Job Discovery Service",
            "Job Platform Scrapers"
          ],
          "existingModules": [
            "job-scraping-engine",
            "job-data-normalization"
          ],
          "schemas": [
            "job-listing-schema",
            "platform-metadata-schema"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/platforms/himalayas.ts",
          "packages/@jobsearch/scrapers/src/platforms/himalayas.test.ts",
          "Updated scraper registry configuration",
          "Himalayas-specific metadata extraction utilities"
        ]
      },
      "riskLevel": "medium",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "Himalayas uses a modern React-based interface with potential API endpoints. Investigate if they expose public APIs before implementing HTML scraping. Handle dynamic loading with appropriate wait strategies and implement fallbacks for JavaScript-disabled scenarios."
    },
    {
      "id": "FBS-010",
      "title": "Indeed Job Scraper Implementation",
      "summary": "Implements a dedicated scraper module for Indeed job platform with HTTP request handling, data extraction, and duplicate detection. Follows the established scraper pattern from previous platform implementations with Indeed-specific parsing logic.",
      "storyScope": [
        {
          "usId": "US-005",
          "acIds": [
            "AC-018",
            "AC-019",
            "AC-020",
            "AC-021"
          ]
        }
      ],
      "dependencies": [
        "FBS-006",
        "FBS-007"
      ],
      "testableOutcomes": [
        "Scraper successfully retrieves Indeed job listings page via HTTP requests without errors",
        "System extracts job title, company, description, location, and posting date from each Indeed listing",
        "Duplicate jobs with same title and company are identified and not re-stored in database",
        "Each stored job record includes the source platform identifier 'indeed'",
        "Indeed scraper is registered and callable through the scraping orchestration service"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 5,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery Service"
          ],
          "tadSections": [
            "Job Platform Scrapers"
          ],
          "existingModules": [
            "packages/@jobsearch/scrapers",
            "scraper registry"
          ],
          "schemas": [
            "job data schema",
            "platform configuration"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/platforms/indeed.ts",
          "packages/@jobsearch/scrapers/src/platforms/indeed.test.ts",
          "Updated scraper registry configuration",
          "Indeed-specific data mapping utilities"
        ]
      },
      "riskLevel": "high",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "Indeed has sophisticated anti-bot measures and may require careful request headers, user agent rotation, and potentially proxy support. The scraper must handle Indeed's specific HTML structure and pagination patterns while respecting their robots.txt and rate limits."
    }
  ],
  "createdAt": "2026-02-18T11:12:42.019Z",
  "updatedAt": "2026-02-18T11:12:42.019Z",
  "partInfo": {
    "partNumber": 1,
    "totalParts": 8,
    "fbsRange": {
      "start": "FBS-001",
      "end": "FBS-010"
    },
    "globalFbsCount": 73
  }
}