{
  "bsId": "BS-002",
  "prdId": "PRD-001",
  "version": "1.0.0",
  "status": "draft",
  "title": "Build Sequence: PRD-001 (Part 2 of 8)",
  "buildPhilosophy": "Generated using vertical-slice strategy. Each FBS represents a testable, deployable increment.",
  "generationStrategy": "vertical-slice",
  "buildSequence": [
    {
      "id": "FBS-011",
      "title": "Rate Limiting & Circuit Breakers",
      "summary": "Implements comprehensive rate limiting and circuit breaker patterns for all job scrapers to handle platform restrictions gracefully. Includes exponential backoff retry logic and circuit breaker state management to maintain platform relationships and system stability.",
      "storyScope": [
        {
          "usId": "US-006",
          "acIds": [
            "AC-022",
            "AC-023",
            "AC-024",
            "AC-025",
            "AC-026"
          ]
        },
        {
          "usId": "US-007",
          "acIds": [
            "AC-027",
            "AC-028",
            "AC-029",
            "AC-030",
            "AC-031"
          ]
        }
      ],
      "dependencies": [
        "FBS-006"
      ],
      "testableOutcomes": [
        "Circuit breaker transitions to 'open' state when scraper receives HTTP 429 rate limit response",
        "Requests are immediately rejected without HTTP calls when circuit breaker is in 'open' state",
        "Circuit breaker transitions to 'half-open' state after configured timeout period expires",
        "Circuit breaker transitions to 'closed' state when test request succeeds in 'half-open' state",
        "Circuit breaker returns to 'open' state when test request fails in 'half-open' state",
        "First retry waits for base delay period (1 second) after retryable error",
        "Subsequent retries double the delay period (1s, 2s, 4s, 8s) with each attempt",
        "Scraping job fails after maximum number of retries (5 attempts) is reached",
        "Maximum delay threshold (60 seconds) is enforced for retry attempts",
        "Non-retryable errors (4xx except 429) fail immediately without retries"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "large",
        "estimatedHours": 7,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery Service"
          ],
          "tadSections": [
            "Job Platform Scrapers"
          ],
          "existingModules": [
            "packages/@jobsearch/scrapers",
            "scraping orchestration"
          ],
          "schemas": [
            "error handling schema",
            "retry configuration"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/circuit-breaker.ts",
          "packages/@jobsearch/scrapers/src/rate-limiter.ts",
          "packages/@jobsearch/scrapers/src/retry-handler.ts",
          "Updated base scraper class with circuit breaker integration",
          "Circuit breaker configuration and monitoring utilities",
          "Comprehensive test suite for all failure scenarios"
        ]
      },
      "riskLevel": "high",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "This is a critical infrastructure component that affects all scrapers. Must be implemented with careful consideration of different failure modes, proper state persistence across service restarts, and comprehensive logging for debugging. Circuit breaker states should be observable through metrics/monitoring."
    },
    {
      "id": "FBS-012",
      "title": "Job Data Extraction Engine",
      "summary": "Implements a robust data extraction engine that normalizes job information across different platforms with consistent field mapping and validation. Provides standardized extraction utilities and handles missing or malformed data gracefully.",
      "storyScope": [
        {
          "usId": "US-009",
          "acIds": [
            "AC-037",
            "AC-038",
            "AC-039",
            "AC-040",
            "AC-041"
          ]
        }
      ],
      "dependencies": [
        "FBS-006"
      ],
      "testableOutcomes": [
        "System extracts job title as non-empty string from standard formatted listings",
        "System extracts company name as non-empty string from listings with company information",
        "System extracts location information including city and state/country when available",
        "Missing fields are marked as null/empty and extraction issues are logged",
        "Basic information extraction achieves at least 90% accuracy across different listing formats",
        "Extracted data is validated against job schema before storage",
        "Data extraction engine handles platform-specific HTML structures consistently"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 6,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery Service"
          ],
          "tadSections": [
            "Job Platform Scrapers"
          ],
          "existingModules": [
            "packages/@jobsearch/scrapers",
            "job data schema"
          ],
          "schemas": [
            "job data schema",
            "validation rules"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/extraction-engine.ts",
          "packages/@jobsearch/scrapers/src/data-normalizer.ts",
          "packages/@jobsearch/scrapers/src/field-extractors/",
          "Data validation and sanitization utilities",
          "Platform-agnostic extraction interface",
          "Comprehensive extraction accuracy test suite"
        ]
      },
      "riskLevel": "medium",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "Focus on creating reusable extraction patterns that can handle the variety of HTML structures across platforms. Include robust text cleaning, date parsing, and location standardization. The engine should be extensible for future platforms and provide detailed extraction metrics for monitoring data quality."
    },
    {
      "id": "FBS-013",
      "title": "Advanced Job Data Extraction",
      "summary": "Implements comprehensive job data extraction capabilities including salary information, full job descriptions, technology/skills identification, and posting date extraction. Provides structured extraction of all job listing components with proper normalization and validation.",
      "storyScope": [
        {
          "usId": "US-010",
          "acIds": [
            "AC-042",
            "AC-043",
            "AC-044",
            "AC-045",
            "AC-046"
          ]
        },
        {
          "usId": "US-011",
          "acIds": [
            "AC-047",
            "AC-048",
            "AC-049",
            "AC-050"
          ]
        },
        {
          "usId": "US-012",
          "acIds": [
            "AC-051",
            "AC-052",
            "AC-053",
            "AC-054",
            "AC-055"
          ]
        },
        {
          "usId": "US-013",
          "acIds": [
            "AC-056",
            "AC-057",
            "AC-058",
            "AC-059",
            "AC-060"
          ]
        }
      ],
      "dependencies": [
        "FBS-012"
      ],
      "testableOutcomes": [
        "System extracts salary ranges with minimum and maximum values including currency from job listings",
        "System extracts single salary amounts and identifies payment frequency (hourly, annual, etc.)",
        "System normalizes salary formats (K, thousands, per year) to standardized numeric values",
        "System extracts complete job description text while preserving paragraph structure",
        "System converts HTML-formatted descriptions to clean text maintaining readability",
        "System identifies and extracts programming languages as structured lists from job descriptions",
        "System categorizes technologies into frameworks, databases, cloud platforms, and tools",
        "System normalizes technology variations (JS/JavaScript, AWS/Amazon Web Services) to standard forms",
        "System extracts experience requirements with both technology and experience level",
        "System converts posting dates to ISO format and handles relative dates (2 days ago)",
        "System extracts and validates application URLs and email addresses",
        "System uses extraction timestamp as fallback when posting date is unavailable"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "large",
        "estimatedHours": 7,
        "contextRequirements": {
          "prdSections": [
            "Job Data Structure",
            "Extraction Requirements"
          ],
          "tadSections": [
            "Job Discovery Service",
            "Job Platform Scrapers"
          ],
          "existingModules": [
            "extraction-engine.ts",
            "data-normalizer.ts",
            "field-extractors/"
          ],
          "schemas": [
            "Job",
            "JobSalary",
            "JobRequirements"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/extractors/salary-extractor.ts",
          "packages/@jobsearch/scrapers/src/extractors/description-extractor.ts",
          "packages/@jobsearch/scrapers/src/extractors/skills-extractor.ts",
          "packages/@jobsearch/scrapers/src/extractors/date-extractor.ts",
          "packages/@jobsearch/scrapers/src/normalizers/technology-normalizer.ts",
          "packages/@jobsearch/scrapers/src/normalizers/salary-normalizer.ts",
          "Enhanced extraction-engine.ts with advanced extractors",
          "Comprehensive test suite for all extraction capabilities"
        ]
      },
      "riskLevel": "medium",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "Requires sophisticated text parsing and NLP techniques for skill extraction. Salary parsing must handle multiple currencies and formats. Technology normalization needs comprehensive mapping database."
    },
    {
      "id": "FBS-014",
      "title": "Extraction Error Handling",
      "summary": "Implements robust error handling for the job data extraction process, including graceful degradation, timeout management, and comprehensive logging. Ensures system stability when processing malformed or problematic job listings.",
      "storyScope": [
        {
          "usId": "US-014",
          "acIds": [
            "AC-061",
            "AC-062",
            "AC-063",
            "AC-064"
          ]
        }
      ],
      "dependencies": [
        "FBS-013"
      ],
      "testableOutcomes": [
        "System logs detailed error information when encountering malformed job listings and continues processing other fields",
        "System creates partial job records with available data when required fields cannot be extracted",
        "System flags missing information in partial records for data quality tracking",
        "System terminates extraction gracefully when processing exceeds configured timeout threshold",
        "System logs timeout events with context about the problematic listing",
        "System alerts administrators when error rate for a source exceeds configured threshold",
        "System temporarily disables problematic sources after repeated extraction failures",
        "System maintains extraction success/failure metrics per platform for monitoring"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "small",
        "estimatedHours": 3,
        "contextRequirements": {
          "prdSections": [
            "Error Handling",
            "System Monitoring"
          ],
          "tadSections": [
            "Job Discovery Service",
            "Notification Service"
          ],
          "existingModules": [
            "extraction-engine.ts",
            "circuit-breaker.ts"
          ],
          "schemas": [
            "ExtractionResult",
            "ErrorLog"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/error-handler.ts",
          "packages/@jobsearch/scrapers/src/extraction-monitor.ts",
          "Enhanced extraction-engine.ts with error handling",
          "Error logging and alerting utilities",
          "Timeout management configuration",
          "Test suite for error scenarios"
        ]
      },
      "riskLevel": "medium",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "Must balance between data completeness and system stability. Error thresholds need careful tuning to avoid false positives while catching real issues."
    },
    {
      "id": "FBS-015",
      "title": "Job Deduplication System",
      "summary": "Implements intelligent job deduplication during ingestion process using case-insensitive title and company matching. Automatically selects the most complete version when duplicates are detected, ensuring data quality and preventing redundant job listings.",
      "storyScope": [
        {
          "usId": "US-015",
          "acIds": [
            "AC-065",
            "AC-066",
            "AC-067",
            "AC-068",
            "AC-069",
            "AC-070"
          ]
        },
        {
          "usId": "US-016",
          "acIds": [
            "AC-071",
            "AC-072",
            "AC-073",
            "AC-074",
            "AC-075"
          ]
        }
      ],
      "dependencies": [
        "FBS-013"
      ],
      "testableOutcomes": [
        "System identifies job listings with identical titles and company names as duplicates using case-insensitive matching",
        "System normalizes whitespace in job titles and company names before duplicate comparison",
        "System treats 'Software Engineer' and 'software engineer' from same company as duplicates",
        "System does not identify jobs with different titles but same company as duplicates",
        "System does not identify jobs with same title but different companies as duplicates",
        "System evaluates duplicate completeness based on number of non-null fields",
        "System retains the job listing with more populated fields when duplicates have different completeness levels",
        "System keeps the job listing with description when one duplicate has description and other doesn't",
        "System retains the most recently ingested listing when duplicates have equal completeness",
        "System discards less complete duplicate listings without storing them in database"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "large",
        "estimatedHours": 6,
        "contextRequirements": {
          "prdSections": [
            "Data Quality",
            "Deduplication Strategy"
          ],
          "tadSections": [
            "Job Discovery Service"
          ],
          "existingModules": [
            "data-normalizer.ts"
          ],
          "schemas": [
            "Job",
            "DuplicateDetectionResult"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/deduplication-engine.ts",
          "packages/@jobsearch/scrapers/src/duplicate-detector.ts",
          "packages/@jobsearch/scrapers/src/completeness-evaluator.ts",
          "packages/@jobsearch/scrapers/src/text-normalizer.ts",
          "Database indexes for efficient duplicate detection",
          "Deduplication metrics and reporting",
          "Comprehensive test suite covering all duplicate scenarios"
        ]
      },
      "riskLevel": "high",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "High risk due to potential data loss if deduplication logic is incorrect. Requires careful database transaction handling and comprehensive testing. Performance optimization needed for large-scale duplicate detection."
    },
    {
      "id": "FBS-016",
      "title": "Search Deduplication & Edge Cases",
      "summary": "Implements advanced deduplication logic for search results with edge case handling for company names, job titles, and data quality scenarios. Ensures only unique job listings appear in search results while properly handling special characters, abbreviations, and malformed data.",
      "storyScope": [
        {
          "usId": "US-017",
          "acIds": [
            "AC-076",
            "AC-077",
            "AC-078",
            "AC-079"
          ]
        },
        {
          "usId": "US-018",
          "acIds": [
            "AC-080",
            "AC-081",
            "AC-082",
            "AC-083",
            "AC-084"
          ]
        }
      ],
      "dependencies": [
        "FBS-015"
      ],
      "testableOutcomes": [
        "Search results display only one instance of jobs with identical title and company combinations",
        "Most complete job version appears when duplicates exist across multiple platforms",
        "Paginated search results contain no duplicate jobs across any page",
        "Job details view shows the most complete version of deduplicated job information",
        "Company names with special characters are normalized for duplicate comparison",
        "Abbreviated company names (IBM) are treated as different from full names (International Business Machines)",
        "Job titles with descriptors (Senior Software Engineer vs Software Engineer - Senior) are treated as different positions",
        "Jobs with null or empty title/company fields are not matched against any other listings",
        "Jobs with identical title/company but different locations are identified as duplicates"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 5,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery",
            "Search & Filtering"
          ],
          "tadSections": [
            "Job Discovery Service"
          ],
          "existingModules": [
            "deduplication-engine",
            "duplicate-detector",
            "text-normalizer"
          ],
          "schemas": [
            "Job",
            "SearchResult"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/search-deduplicator.ts",
          "packages/@jobsearch/scrapers/src/edge-case-handler.ts",
          "Enhanced duplicate-detector.ts with edge case logic",
          "Search result deduplication middleware",
          "Comprehensive test suite for edge cases"
        ]
      },
      "riskLevel": "medium",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "Focus on text normalization algorithms for company names and job titles. Implement fuzzy matching carefully to avoid over-aggressive deduplication. Consider performance impact of complex normalization on search queries."
    },
    {
      "id": "FBS-017",
      "title": "Data Normalization Pipeline",
      "summary": "Implements comprehensive data normalization for job listings including country names, technology names, and salary currency conversion. Maintains original data alongside normalized values for audit and verification purposes.",
      "storyScope": [
        {
          "usId": "US-019",
          "acIds": [
            "AC-085",
            "AC-086",
            "AC-087",
            "AC-088",
            "AC-089"
          ]
        },
        {
          "usId": "US-020",
          "acIds": [
            "AC-090",
            "AC-091",
            "AC-092",
            "AC-093",
            "AC-094",
            "AC-095"
          ]
        },
        {
          "usId": "US-021",
          "acIds": [
            "AC-096",
            "AC-097",
            "AC-098",
            "AC-099",
            "AC-100",
            "AC-101"
          ]
        },
        {
          "usId": "US-024",
          "acIds": [
            "AC-112",
            "AC-113",
            "AC-114",
            "AC-115"
          ]
        }
      ],
      "dependencies": [
        "FBS-013",
        "FBS-014"
      ],
      "testableOutcomes": [
        "Country 'USA' is normalized to 'United States' while preserving original value",
        "Country 'UK' is normalized to 'United Kingdom' while preserving original value",
        "Country 'Deutschland' is normalized to 'Germany' while preserving original value",
        "Unrecognized country names are preserved unchanged",
        "Technology 'React.js' and 'ReactJS' are both normalized to 'React'",
        "Technology 'Node.js' maintains its original form after normalization",
        "Case variations 'PYTHON' and 'python' are normalized to 'Python'",
        "Unrecognized technology names are preserved unchanged",
        "EUR salaries are converted to USD using current exchange rates",
        "GBP salaries are converted to USD using current exchange rates",
        "USD salaries remain unchanged during normalization",
        "Salary ranges in foreign currencies have both min and max converted to USD",
        "Unrecognized currency codes preserve original salary information",
        "Both original and normalized values are accessible in job records",
        "Normalization timestamps are recorded for audit purposes"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "large",
        "estimatedHours": 7,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery",
            "Data Management"
          ],
          "tadSections": [
            "Job Discovery Service"
          ],
          "existingModules": [
            "data-normalizer",
            "salary-extractor",
            "extraction-engine"
          ],
          "schemas": [
            "Job",
            "NormalizedJob",
            "SalaryInfo"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/normalizers/country-normalizer.ts",
          "packages/@jobsearch/scrapers/src/normalizers/technology-normalizer.ts",
          "packages/@jobsearch/scrapers/src/normalizers/currency-converter.ts",
          "Enhanced data-normalizer.ts with comprehensive pipeline",
          "Normalization mapping configuration files",
          "Exchange rate service integration",
          "Audit trail utilities",
          "Comprehensive normalization test suite"
        ]
      },
      "riskLevel": "high",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "External API dependency for currency conversion introduces risk. Implement caching and fallback mechanisms. Consider data migration strategy for existing job records. Performance optimization needed for bulk normalization operations."
    },
    {
      "id": "FBS-018",
      "title": "Experience Level Categorization & Batch Processing",
      "summary": "Implements experience level categorization from job requirements text and provides efficient batch processing capabilities for high-volume job normalization operations with error handling and reporting.",
      "storyScope": [
        {
          "usId": "US-022",
          "acIds": [
            "AC-102",
            "AC-103",
            "AC-104",
            "AC-105",
            "AC-106",
            "AC-107"
          ]
        },
        {
          "usId": "US-023",
          "acIds": [
            "AC-108",
            "AC-109",
            "AC-110",
            "AC-111"
          ]
        }
      ],
      "dependencies": [
        "FBS-017"
      ],
      "testableOutcomes": [
        "Jobs requiring '0-2 years experience' are categorized as 'Entry Level'",
        "Jobs requiring '3-5 years experience' are categorized as 'Mid Level'",
        "Jobs requiring '6+ years experience' are categorized as 'Senior Level'",
        "Jobs mentioning 'Junior Developer' are categorized as 'Entry Level'",
        "Jobs mentioning 'Senior Engineer' are categorized as 'Senior Level'",
        "Jobs with no clear experience indicators are set to 'Not Specified'",
        "Batch of 100 jobs processes within 30 seconds",
        "Single job failure in batch does not prevent other jobs from processing successfully",
        "Batch processing generates summary report with success and failure counts",
        "Concurrent batch operations maintain data integrity across all batches"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 5,
        "contextRequirements": {
          "prdSections": [
            "Job Discovery",
            "Data Management"
          ],
          "tadSections": [
            "Job Discovery Service"
          ],
          "existingModules": [
            "data-normalizer",
            "extraction-engine",
            "error-handler"
          ],
          "schemas": [
            "Job",
            "BatchProcessingResult",
            "ExperienceLevel"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/scrapers/src/normalizers/experience-categorizer.ts",
          "packages/@jobsearch/scrapers/src/batch-processor.ts",
          "packages/@jobsearch/scrapers/src/batch-reporter.ts",
          "Enhanced data-normalizer.ts with experience level detection",
          "Experience level pattern matching configuration",
          "Batch processing queue management",
          "Performance monitoring utilities",
          "Batch processing test suite"
        ]
      },
      "riskLevel": "medium",
      "domain": "Job Discovery & Ingestion",
      "implementationNotes": "Text pattern matching for experience levels requires careful regex design to avoid false positives. Batch processing needs memory management for large datasets. Consider implementing job queue system for scalability."
    },
    {
      "id": "FBS-019",
      "title": "CV Upload & Text Extraction",
      "summary": "Implements secure file upload functionality for PDF and DOCX CV files with comprehensive text extraction capabilities. Provides robust error handling for file validation, size limits, and extraction failures with clear user feedback.",
      "storyScope": [
        {
          "usId": "US-032",
          "acIds": [
            "AC-151",
            "AC-152",
            "AC-153",
            "AC-154",
            "AC-155"
          ]
        },
        {
          "usId": "US-033",
          "acIds": [
            "AC-156",
            "AC-157",
            "AC-158",
            "AC-159",
            "AC-160"
          ]
        },
        {
          "usId": "US-037",
          "acIds": [
            "AC-176",
            "AC-177",
            "AC-178",
            "AC-179",
            "AC-180"
          ]
        }
      ],
      "dependencies": [
        "FBS-004"
      ],
      "testableOutcomes": [
        "User can successfully upload PDF files under 5MB and receive confirmation message",
        "User can successfully upload DOCX files under 5MB and receive confirmation message",
        "System rejects files over 5MB with error message 'File size must be under 5MB'",
        "System rejects non-PDF/DOCX files with error message 'Only PDF and DOCX formats are supported'",
        "System extracts at least 95% of visible text from PDF files accurately",
        "System extracts at least 95% of document text from DOCX files accurately",
        "System preserves special characters and formatting in extracted text",
        "System combines text from all pages in multi-page CV documents",
        "System returns appropriate error message when CV file is corrupted or unreadable",
        "System displays helpful error message for poor quality scanned CVs suggesting better quality document",
        "System rejects password-protected files with clear error message",
        "System displays retry option when parsing encounters timeout or system errors"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "large",
        "estimatedHours": 7,
        "contextRequirements": {
          "prdSections": [
            "User Profile Management",
            "File Upload Security"
          ],
          "tadSections": [
            "User Profile Service"
          ],
          "existingModules": [
            "Authentication middleware",
            "Environment configuration"
          ],
          "schemas": [
            "User profile schema",
            "File metadata schema"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/cv-parser/src/file-uploader.ts",
          "packages/@jobsearch/cv-parser/src/text-extractor.ts",
          "packages/@jobsearch/cv-parser/src/pdf-parser.ts",
          "packages/@jobsearch/cv-parser/src/docx-parser.ts",
          "packages/@jobsearch/cv-parser/src/validation.ts",
          "packages/@jobsearch/cv-parser/src/error-handler.ts",
          "apps/api-server/src/routes/cv-upload.ts",
          "File upload API endpoints with validation",
          "Text extraction service integration",
          "Comprehensive test suite for all file formats and error cases"
        ]
      },
      "riskLevel": "high",
      "domain": "User Profile & CV Management",
      "implementationNotes": "High risk due to file parsing complexity, security considerations for file uploads, and dependency on external libraries for PDF/DOCX processing. Requires careful memory management for large files and robust error handling for various file corruption scenarios."
    },
    {
      "id": "FBS-020",
      "title": "CV Skills & Experience Extraction",
      "summary": "Implements intelligent extraction of technical skills, work experience, and professional details from CV text content. Uses pattern matching and NLP techniques to identify programming languages, frameworks, tools, job positions, and calculate experience duration.",
      "storyScope": [
        {
          "usId": "US-034",
          "acIds": [
            "AC-161",
            "AC-162",
            "AC-163",
            "AC-164",
            "AC-165"
          ]
        },
        {
          "usId": "US-035",
          "acIds": [
            "AC-166",
            "AC-167",
            "AC-168",
            "AC-169",
            "AC-170"
          ]
        }
      ],
      "dependencies": [
        "FBS-019"
      ],
      "testableOutcomes": [
        "System identifies and extracts programming languages (Python, JavaScript, Java) from CV text",
        "System identifies and categorizes frameworks and libraries (React, Django, Spring) appropriately",
        "System identifies and extracts tools and platforms (AWS, Docker, Git) from CV content",
        "System identifies and categorizes database technologies (MySQL, PostgreSQL, MongoDB)",
        "System normalizes extracted technical skills to standard names and versions",
        "System calculates accurate duration for job positions with start and end dates",
        "System calculates duration from start date to current date for positions marked as 'Present' or 'Current'",
        "System extracts job titles and company names for each employment position",
        "System accounts for concurrent roles when calculating total experience from overlapping periods",
        "System calculates and provides total years of experience across all positions"
      ],
      "status": "not-started",
      "sessionMeta": {
        "estimatedSize": "medium",
        "estimatedHours": 5,
        "contextRequirements": {
          "prdSections": [
            "Skills Analysis",
            "Experience Calculation"
          ],
          "tadSections": [
            "User Profile Service",
            "Intelligent Matching Service"
          ],
          "existingModules": [
            "Text extraction utilities",
            "Data normalization pipeline"
          ],
          "schemas": [
            "Skills schema",
            "Experience schema",
            "Job position schema"
          ]
        },
        "deliverables": [
          "packages/@jobsearch/cv-parser/src/skills-extractor.ts",
          "packages/@jobsearch/cv-parser/src/experience-extractor.ts",
          "packages/@jobsearch/cv-parser/src/pattern-matchers/",
          "packages/@jobsearch/cv-parser/src/normalizers/skills-normalizer.ts",
          "packages/@jobsearch/cv-parser/src/date-calculator.ts",
          "packages/@jobsearch/cv-parser/src/experience-calculator.ts",
          "Skills and frameworks recognition patterns",
          "Experience duration calculation logic",
          "Comprehensive test suite for extraction accuracy"
        ]
      },
      "riskLevel": "medium",
      "domain": "User Profile & CV Management",
      "implementationNotes": "Medium risk due to variability in CV formats and need for accurate pattern matching. Requires extensive testing with diverse CV samples and may need machine learning approaches for improved accuracy in skill recognition."
    }
  ],
  "createdAt": "2026-02-18T11:12:42.019Z",
  "updatedAt": "2026-02-18T11:12:42.019Z",
  "partInfo": {
    "partNumber": 2,
    "totalParts": 8,
    "fbsRange": {
      "start": "FBS-011",
      "end": "FBS-020"
    },
    "globalFbsCount": 73
  }
}